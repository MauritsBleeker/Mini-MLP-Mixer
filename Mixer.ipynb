{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patcher(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_patches, dim_in=3, channels=512, patch_size=16,):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.projection = nn.Conv2d(3, channels, kernel_size=patch_size, stride=(patch_size, patch_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.projection(x).flatten(2).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, channels):\n",
    "                \n",
    "        super().__init__()\n",
    "\n",
    "        self.projection = nn.Linear(channels, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.projection(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, num_patches):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        \n",
    "        self.mlp1 = MLP(num_patches, num_patches)\n",
    "        self.mlp2 = MLP(channels, channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + mlp1(norm1(x).transpose(2,1)).transpose(2,1)\n",
    "        x = x + self.mlp2(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixer(nn.Module):\n",
    "    \n",
    "    def __init__(self, n, channels, num_patches, n_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            MixerLayer(channels, num_patches)\n",
    "            for _ in range(n)])\n",
    "        \n",
    "        self.pooling = PoolingLayer(n_classes, channels)\n",
    "        \n",
    "        self.patcher = Patcher(num_patches)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patcher(x)\n",
    "        x = self.blocks(x)\n",
    "        out = self.pooling(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 128\n",
    "h = 224\n",
    "w = 224\n",
    "c_in = 3\n",
    "\n",
    "x = torch.rand(b, c_in, h, w)\n",
    "\n",
    "patch_size = 16\n",
    "channels = 512\n",
    "dim_in = 3\n",
    "n_classes = 10\n",
    "n = 6 \n",
    "num_patches = int(h/patch_size) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "patcher = Patcher(num_patches)\n",
    "layer = MixerLayer(channels=channels, num_patches=num_patches)\n",
    "pooling = PoolingLayer(n_classes, channels)\n",
    "\n",
    "mixer = Mixer(n, channels, num_patches, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = patcher(x)\n",
    "z = layer(z)\n",
    "out  = pooling(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = mixer(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
