{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patcher(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_patches, dim_in=3, channels=512, patch_size=16,):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.projection = nn.Conv2d(3, channels, kernel_size=patch_size, stride=(patch_size, patch_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.projection(x).flatten(2).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, channels):\n",
    "                \n",
    "        super().__init__()\n",
    "\n",
    "        self.projection = nn.Linear(channels, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.projection(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, num_patches, dropout=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        \n",
    "        self.mlp1 = MLP(num_patches, num_patches, dropout)\n",
    "        self.mlp2 = MLP(channels, channels, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.mlp1(self.norm1(x).transpose(2,1)).transpose(2,1)\n",
    "        x = x + self.mlp2(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixer(nn.Module):\n",
    "    \n",
    "    def __init__(self, n, channels, num_patches, n_classes, dropout=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            MixerLayer(channels, num_patches, dropout=0.1)\n",
    "            for _ in range(n)])\n",
    "        \n",
    "        self.pooling = PoolingLayer(n_classes, channels)\n",
    "        \n",
    "        self.patcher = Patcher(num_patches)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.patcher(x)\n",
    "        z = self.blocks(z)\n",
    "        logist = self.pooling(z)\n",
    "        \n",
    "        return logist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 128\n",
    "h = 224\n",
    "w = 224\n",
    "c_in = 3\n",
    "\n",
    "x = torch.rand(b, c_in, h, w)\n",
    "\n",
    "patch_size = 16\n",
    "channels = 512\n",
    "dim_in = 3\n",
    "n_classes = 10\n",
    "n = 6 \n",
    "num_patches = int(h/patch_size) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patcher = Patcher(num_patches)\n",
    "layer1 = MixerLayer(channels=channels, num_patches=num_patches)\n",
    "layer2 = MixerLayer(channels=channels, num_patches=num_patches)\n",
    "pooling = PoolingLayer(n_classes, channels)\n",
    "\n",
    "mixer = Mixer(n, channels, num_patches, n_classes, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = patcher(x)\n",
    "z = layer1(z)\n",
    "z = layer2(z)\n",
    "logist  = pooling(z)\n",
    "logist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logist = mixer(x)\n",
    "logist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
